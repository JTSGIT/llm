{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T01:24:02.165848Z",
     "start_time": "2024-07-09T01:24:00.842127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import the necessary class\n",
    "from transformers import DistilBertTokenizer"
   ],
   "id": "6beaa68022c84e84",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-09T01:24:02.560210Z",
     "start_time": "2024-07-09T01:24:02.166797Z"
    }
   },
   "source": [
    "#Tokenize a Sentence\n",
    "\n",
    "# Load the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T01:24:02.562401Z",
     "start_time": "2024-07-09T01:24:02.560876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example sentence\n",
    "sentence = 'Tokenize this sentence using DistilBERT.'"
   ],
   "id": "ad1a96f1613f8731",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T01:24:02.567650Z",
     "start_time": "2024-07-09T01:24:02.563280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenize the sentence\n",
    "tokens = tokenizer(sentence, return_tensors='pt')"
   ],
   "id": "14754bc987b10a4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T01:24:02.570912Z",
     "start_time": "2024-07-09T01:24:02.568985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrieve the input_ids\n",
    "input_ids = tokens['input_ids'].squeeze().tolist()"
   ],
   "id": "4d901824a7562c51",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T01:24:02.573309Z",
     "start_time": "2024-07-09T01:24:02.571537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrieve the attention mask\n",
    "attention_mask = tokens['attention_mask'].squeeze().tolist()"
   ],
   "id": "851ac85e8b993382",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T01:24:02.575644Z",
     "start_time": "2024-07-09T01:24:02.573848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print the results\n",
    "print(f'Original sentence:\\n{sentence}\\n')\n",
    "print(f'Input IDs:\\n{input_ids}\\n')\n",
    "print(f'Attention mask:\\n{attention_mask}')"
   ],
   "id": "f1cdaf8141b868b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      "Tokenize this sentence using DistilBERT.\n",
      "\n",
      "Input IDs:\n",
      "[101, 19204, 4697, 2023, 6251, 2478, 4487, 16643, 23373, 1012, 102]\n",
      "\n",
      "Attention mask:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T01:24:02.578341Z",
     "start_time": "2024-07-09T01:24:02.576429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Map each input id to its corresponding token\n",
    "for input_id in input_ids:\n",
    "    print(f'{input_id}: {tokenizer.decode([input_id])}')"
   ],
   "id": "dd6cfb35bfa48084",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101: [CLS]\n",
      "19204: token\n",
      "4697: ##ize\n",
      "2023: this\n",
      "6251: sentence\n",
      "2478: using\n",
      "4487: di\n",
      "16643: ##sti\n",
      "23373: ##lbert\n",
      "1012: .\n",
      "102: [SEP]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T01:24:26.399561Z",
     "start_time": "2024-07-09T01:24:26.392161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Additional sample sentences\n",
    "sample_sentences = [\n",
    "    \"Let's see how DistilBERT handles this sentence.\",\n",
    "    \"Adding a nonsensical word like blorbz to see what happens.\",\n",
    "    \"How does it handle emojis? ðŸ˜ŠðŸš€\"\n",
    "]\n",
    "\n",
    "for sentence in sample_sentences:\n",
    "    tokens = tokenizer(sentence, return_tensors='pt')\n",
    "    input_ids = tokens['input_ids'].squeeze().tolist()\n",
    "    attention_mask = tokens['attention_mask'].squeeze().tolist()\n",
    "    \n",
    "    print(f'\\nOriginal sentence:\\n{sentence}\\n')\n",
    "    print(f'Input IDs:\\n{input_ids}\\n')\n",
    "    print(f'Attention mask:\\n{attention_mask}\\n')\n",
    "    \n",
    "    for input_id in input_ids:\n",
    "        print(f'{input_id}: {tokenizer.decode([input_id])}')"
   ],
   "id": "8258bf7234a15327",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original sentence:\n",
      "Let's see how DistilBERT handles this sentence.\n",
      "\n",
      "Input IDs:\n",
      "[101, 2292, 1005, 1055, 2156, 2129, 4487, 16643, 23373, 16024, 2023, 6251, 1012, 102]\n",
      "\n",
      "Attention mask:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "101: [CLS]\n",
      "2292: let\n",
      "1005: '\n",
      "1055: s\n",
      "2156: see\n",
      "2129: how\n",
      "4487: di\n",
      "16643: ##sti\n",
      "23373: ##lbert\n",
      "16024: handles\n",
      "2023: this\n",
      "6251: sentence\n",
      "1012: .\n",
      "102: [SEP]\n",
      "\n",
      "Original sentence:\n",
      "Adding a nonsensical word like blorbz to see what happens.\n",
      "\n",
      "Input IDs:\n",
      "[101, 5815, 1037, 2512, 5054, 19570, 2389, 2773, 2066, 1038, 10626, 2497, 2480, 2000, 2156, 2054, 6433, 1012, 102]\n",
      "\n",
      "Attention mask:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "101: [CLS]\n",
      "5815: adding\n",
      "1037: a\n",
      "2512: non\n",
      "5054: ##sen\n",
      "19570: ##sic\n",
      "2389: ##al\n",
      "2773: word\n",
      "2066: like\n",
      "1038: b\n",
      "10626: ##lor\n",
      "2497: ##b\n",
      "2480: ##z\n",
      "2000: to\n",
      "2156: see\n",
      "2054: what\n",
      "6433: happens\n",
      "1012: .\n",
      "102: [SEP]\n",
      "\n",
      "Original sentence:\n",
      "How does it handle emojis? ðŸ˜ŠðŸš€\n",
      "\n",
      "Input IDs:\n",
      "[101, 2129, 2515, 2009, 5047, 7861, 29147, 2483, 1029, 100, 102]\n",
      "\n",
      "Attention mask:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "101: [CLS]\n",
      "2129: how\n",
      "2515: does\n",
      "2009: it\n",
      "5047: handle\n",
      "7861: em\n",
      "29147: ##oj\n",
      "2483: ##is\n",
      "1029: ?\n",
      "100: [UNK]\n",
      "102: [SEP]\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
